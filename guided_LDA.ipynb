{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from gensim.test.utils import datapath\n",
    "import tunning_lda as tlda \n",
    "%matplotlib inline\n",
    "\n",
    "regex_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Readding and cleaning the training data\n",
    "\n",
    "This process is different depending on the input data, in this case, the data contained weird characters and some of the observations were error messages, among some other particularities. Because of that the cleaning process is done in the following lines intead of using the cleaning functions used in other parts of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading training data\n",
    "data = pd.read_csv('text_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These documents does not contain useful text\n",
    "not_included = [642, 569, 567, 582, 595, 597, 611, 640, 636, 598]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a dictionary that has a unique id as key and raw text as value\n",
    "corpus_dic = {r['domain_url']:r['text'] for i, r in data.iterrows() if r['domain_url'] not in not_included}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English stop words\n",
    "sw = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In these lines we tokenize the text, eliminate punctuation and convert all characters to lower\n",
    "original_corpus = [regex_tokenizer.tokenize(doc) for doc in corpus_dic.values()]\n",
    "original_corpus = [[t.lower() for t in doc if t not in sw] for doc in original_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we create the original vocabulary of the corpus\n",
    "original_dictionary = gensim.corpora.Dictionary(original_corpus)\n",
    "original_vocab = [original_dictionary[i] for i in range(len(original_dictionary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we define the words that are too short or contain rare characters or numbers.\n",
    "short = [w for w in original_vocab if len(w) <3]\n",
    "rare =[w for w in original_vocab if re.match(\"[\\d=@}#;%`>*'{):~,+|!/_<?\\\\(.&-]\", w)]\n",
    "invalid = short+rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we create a clean corpus, dictionary and bag of words\n",
    "corpus = [[t for t in doc if t not in invalid] for doc in original_corpus]\n",
    "corpus = [[stemmer.stem(t) for t in doc] for doc in corpus]\n",
    "dictionary = gensim.corpora.Dictionary(corpus)\n",
    "bow = [dictionary.doc2bow(line) for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also create a TF-IDF bow, as an alternative to common bag of words\n",
    "TFIDF = TfidfModel(bow)\n",
    "tf_idf = [TFIDF[b] for b in bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing different models\n",
    "\n",
    "We are using the LDA model, but this model could take different parameters, the most important is the number of topics, but we can also change the number of iterations and passses throgh the data and the decay rate.\n",
    "The function called \"dif_models\" simplifies this process, you only have to provide the vectors of the documents considered, the dictionary created previously and a list of different values for every parameter. This funtion returns a data frame that summarizes the results of the different models, the best model (according to coherence) and the topics top terms of the best model.\n",
    "\n",
    "The performance of every model is evaluated using log-perplexity (the lower, the better) and coherence measure (the higher, the better), these two mettrics are reported in the summary data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists that contains different values for the parameters of the LDA model.\n",
    "n_list = [3, 5, 10, 20, 25]\n",
    "iter_list = [200]\n",
    "pass_list = [150]\n",
    "decay_list = [0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 LDA model using simple bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Topics</th>\n",
       "      <th>Iterations</th>\n",
       "      <th>Passes</th>\n",
       "      <th>Decay</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-7.187546</td>\n",
       "      <td>-0.342533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-7.363833</td>\n",
       "      <td>-0.421302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-7.193414</td>\n",
       "      <td>-1.177357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-7.165929</td>\n",
       "      <td>-1.237508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-7.199592</td>\n",
       "      <td>-2.113676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Topics  Iterations  Passes  Decay  Perplexity  Coherence\n",
       "1                 5         200     150    0.7   -7.187546  -0.342533\n",
       "0                 3         200     150    0.7   -7.363833  -0.421302\n",
       "4                25         200     150    0.7   -7.193414  -1.177357\n",
       "2                10         200     150    0.7   -7.165929  -1.237508\n",
       "3                20         200     150    0.7   -7.199592  -2.113676"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing model with different parameters\n",
    "results_df, best_model, best_topics = tlda.dif_models(bow, dictionary, n_list, iter_list, pass_list, decay_list)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "princeton, hackprinceton, travel, student, train, about, workshop, team, hardwar, faq\n",
      "**************************************************************************************************\n",
      "Topic  1\n",
      "hackni, fellow, hackathon, student, startup, work, post, univers, new, the\n",
      "**************************************************************************************************\n",
      "Topic  2\n",
      "hack, team, hackathon, event, sponsor, student, project, what, email, provid\n",
      "**************************************************************************************************\n",
      "Topic  3\n",
      "univers, event, team, host, hackathon, learn, america, east, student, the\n",
      "**************************************************************************************************\n",
      "Topic  4\n",
      "builtworld, compani, industri, construct, member, technolog, the, build, confer, built\n",
      "**************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Exploring the content of each topic of the best model\n",
    "tlda.print_topics(best_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 LDA model using TF-IDF bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jesus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\ldamodel.py:824: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    }
   ],
   "source": [
    "tfidf_results_df, tfidf_best_model, tfidf_best_topics = tlda.dif_models(tf_idf, dictionary, n_list, iter_list, pass_list, decay_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Topics</th>\n",
       "      <th>Iterations</th>\n",
       "      <th>Passes</th>\n",
       "      <th>Decay</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-28.976312</td>\n",
       "      <td>-8.674017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-24.678418</td>\n",
       "      <td>-10.647248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-13.859426</td>\n",
       "      <td>-18.348740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-12.106470</td>\n",
       "      <td>-22.725884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-10.932700</td>\n",
       "      <td>-23.616555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Topics  Iterations  Passes  Decay  Perplexity  Coherence\n",
       "4                25         200     150    0.7  -28.976312  -8.674017\n",
       "3                20         200     150    0.7  -24.678418 -10.647248\n",
       "2                10         200     150    0.7  -13.859426 -18.348740\n",
       "1                 5         200     150    0.7  -12.106470 -22.725884\n",
       "0                 3         200     150    0.7  -10.932700 -23.616555"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "hackric, vcu, ramhack, citizen, rice, field, privaci, fill, virginia, commonwealth\n",
      "**************************************************************************************************\n",
      "Topic  1\n",
      "cgs, techtogeth, boston, wayfair, shehack, prehack, aggani, beginn, host, women\n",
      "**************************************************************************************************\n",
      "Topic  2\n",
      "stanciu, scherocman, synthes, swag, ste, tatyana, shestopalova, parekh, prev, theo\n",
      "**************************************************************************************************\n",
      "Topic  3\n",
      "cypher, swem, gregg, snacki, earl, python, mari, alexa, mushroom, whereowar\n",
      "**************************************************************************************************\n",
      "Topic  4\n",
      "revolutionuc, hackxx, cuhackit, demonhack, houston, coder, helloworld, cougarc, voyag, cloudflar\n",
      "**************************************************************************************************\n",
      "Topic  5\n",
      "hacknc, slo, desk, front, chumash, carpool, cloudflar, mile, everywher, grant\n",
      "**************************************************************************************************\n",
      "Topic  6\n",
      "stanciu, scherocman, synthes, swag, ste, tatyana, shestopalova, parekh, prev, theo\n",
      "**************************************************************************************************\n",
      "Topic  7\n",
      "hackher413, howdyhack, tamuhack, davi, aggi, pdt, hackdavi, post, log, renu\n",
      "**************************************************************************************************\n",
      "Topic  8\n",
      "hackcu, chung, healthhack, uottahack, rose, vcu, ottawa, medic, abhay, xunxun\n",
      "**************************************************************************************************\n",
      "Topic  9\n",
      "uofthack, manila, fake, atrium, philippin, disinform, jakarta, bahen, cube, pancak\n",
      "**************************************************************************************************\n",
      "Topic  10\n",
      "madhack, imposs, tier, hesit, wacki, daredevil, genius, downright, globe, taker\n",
      "**************************************************************************************************\n",
      "Topic  11\n",
      "builtworld, hackpsu, site, privaci, construct, user, parti, cooki, collect, busi\n",
      "**************************************************************************************************\n",
      "Topic  12\n",
      "pennapp, minnehack, hackumass, umn, cloudflar, protect, penn, dine, acm, inacubicl\n",
      "**************************************************************************************************\n",
      "Topic  13\n",
      "hacktech, hoya, hackdavi, georgetown, davi, caltech, forens, nbsp, social, cloudflar\n",
      "**************************************************************************************************\n",
      "Topic  14\n",
      "calvinhack, lobbi, main, calvin, devo, bytwerk, potbelli, room, theater, slack\n",
      "**************************************************************************************************\n",
      "Topic  15\n",
      "makeharvard, xdhack, vcu, earthhack, ubc, makeathon, soch, virginia, path, commonwealth\n",
      "**************************************************************************************************\n",
      "Topic  16\n",
      "north, hackthenorth, waterloo, founder, septemb, canada, ceo, sept, communitech, press\n",
      "**************************************************************************************************\n",
      "Topic  17\n",
      "stanciu, scherocman, synthes, swag, ste, tatyana, shestopalova, parekh, prev, theo\n",
      "**************************************************************************************************\n",
      "Topic  18\n",
      "technica, sbuhack, lehman, hackprinceton, princeton, hackharvard, stroke, paint, nsbe, oct\n",
      "**************************************************************************************************\n",
      "Topic  19\n",
      "qhack, wichack, rit, queen, cloudflar, hall, women, protect, elli, male\n",
      "**************************************************************************************************\n",
      "Topic  20\n",
      "mchack, grizzhack, codestel, ucsb, aeac, womxnhack, lowel, east, america, aauw\n",
      "**************************************************************************************************\n",
      "Topic  21\n",
      "knight, delta, sachack, chicago, deltahack, windi, lasall, cloudflar, blockchain, citi\n",
      "**************************************************************************************************\n",
      "Topic  22\n",
      "uncommon, polski, chicago, uchicago, uncommonhack, celeri, dev, exchang, pennapp, bye\n",
      "**************************************************************************************************\n",
      "Topic  23\n",
      "enghack, hackutd, hubweek, hackduk, mit, schiciano, track, agenda, hudson, decis\n",
      "**************************************************************************************************\n",
      "Topic  24\n",
      "hackni, medhack, hackcoop, ugahack, conuhack, bigr, password, privaci, concordia, term\n",
      "**************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tlda.print_topics(tfidf_best_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Conclusions\n",
    "\n",
    "After testing different models it is possible to decide which model is better, in this case the LDA model with 5 topics using simple bag of words is the one with best performance. Despite its performance, it is possible that the topics produced by the best model are not easy to interpret, it is possible to provide guidance to the model to make the traing process focus in some predefined terms and improve the interpretation of the topics. This is made in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Guided LDA model\n",
    "\n",
    "The gensim LDA model allows us to create a matrix that give more weight to defined terms in certain topics. By doing this, we can guide the model and produce more choherence topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining importants terms and its topics. \n",
    "The key is the term and the value is the number of the topic where the term is assigned.\n",
    "'''\n",
    "apriori_terms = {\n",
    "    'sponsoring': 2,\n",
    "    'sponsors': 2,\n",
    "    'sponsorship': 2,\n",
    "    'sponsor': 2,\n",
    "    'sponsored': 2,\n",
    "    'sponsorships': 2,\n",
    "    'promoter': 2,\n",
    "    'benefactor': 2, \n",
    "    'funding':2,\n",
    "    'aid':2,\n",
    "    'organizer':2,\n",
    "    'help':2,\n",
    "    'support':2\n",
    "}\n",
    "\n",
    "#We use the same stemmer that we previously used in the cleaning process\n",
    "apriori_terms = {stemmer.stem(k): v for k,v in apriori_terms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a model with the parameters of the best model but including the \n",
    "bm = tlda.make_LDA(bow, dictionary, 5, apriori_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the topics and computing the perplexity and coherence of that model\n",
    "bm_topics, bm_perplexity, bm_coherence = tlda.evaluate_model(bm, bow, dictionary, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "princeton, hackprinceton, travel, student, train, about, workshop, team, hardwar, faq\n",
      "**************************************************************************************************\n",
      "Topic  1\n",
      "hackni, fellow, hackathon, student, startup, work, post, univers, new, the\n",
      "**************************************************************************************************\n",
      "Topic  2\n",
      "hack, team, hackathon, event, sponsor, student, project, what, email, provid\n",
      "**************************************************************************************************\n",
      "Topic  3\n",
      "univers, event, team, host, hackathon, learn, america, east, student, the\n",
      "**************************************************************************************************\n",
      "Topic  4\n",
      "builtworld, compani, industri, construct, member, technolog, the, build, confer, built\n",
      "**************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tlda.print_topics(bm_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model and the dictionary to use it to evaluate fresh texts latter.\n",
    "tlda.save_model(bm, 'best_model')\n",
    "tlda.save_model(dictionary, 'best_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
